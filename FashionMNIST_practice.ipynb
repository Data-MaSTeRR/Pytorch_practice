{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (1.0, ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_root = 'C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.1-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp39-cp39-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.1-cp39-cp39-win_amd64.whl.metadata (165 kB)\n",
      "     ---------------------------------------- 0.0/165.9 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 30.7/165.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 165.9/165.9 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\skt01\\anaconda3\\envs\\torch_book\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\skt01\\anaconda3\\envs\\torch_book\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\skt01\\anaconda3\\envs\\torch_book\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\skt01\\anaconda3\\envs\\torch_book\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\skt01\\anaconda3\\envs\\torch_book\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\skt01\\anaconda3\\envs\\torch_book\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.1-cp39-cp39-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.0 MB 20.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.0/8.0 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.4/8.0 MB 26.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.6/8.0 MB 19.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.4/8.0 MB 23.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.0 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp39-cp39-win_amd64.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.8/182.8 kB ? eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.7/2.2 MB 36.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 34.8 MB/s eta 0:00:00\n",
      "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading kiwisolver-1.4.5-cp39-cp39-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.2/56.2 kB ? eta 0:00:00\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 importlib-resources-6.4.0 kiwisolver-1.4.5 matplotlib-3.9.1 pyparsing-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_root = 'C:/Users/SKT01/Desktop/강의자료/Pytorch with AI/AI융합프로그램/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(download_root, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = torchvision.datasets.FashionMNIST(download_root, download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: 'T-Shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionDNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionDNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "model = FashionDNN()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SKT01\\anaconda3\\envs\\torch_book\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    train = Variable(images.view(images.size(0), 1, 28, 28))\n",
    "    labels = Variable(labels)\n",
    "\n",
    "    outputs = model(train)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    count += 1\n",
    "\n",
    "    if not (count % 50):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels_list.append(labels)\n",
    "            test = Variable(images.view(images.size(0), 1, 28, 28))\n",
    "            outputs = model(test)\n",
    "            predictions = torch.max(outputs, 1)[1].to(device)\n",
    "            predictions_list.append(predictions)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "        \n",
    "        accuracy = correct * 100 / total\n",
    "        loss_list.append(loss.data)\n",
    "        iteration_list.append(count)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    if not (count % 500):\n",
    "        print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50, Loss: 0.42750126123428345, Accuracy: 79.89%\n",
      "Iteration: 100, Loss: 0.475765585899353, Accuracy: 84.51%\n",
      "Iteration: 150, Loss: 0.42978447675704956, Accuracy: 84.5%\n",
      "Iteration: 200, Loss: 0.43274906277656555, Accuracy: 85.19%\n",
      "Iteration: 250, Loss: 0.4499438405036926, Accuracy: 83.4%\n",
      "Iteration: 300, Loss: 0.30584055185317993, Accuracy: 87.62%\n",
      "Iteration: 350, Loss: 0.3765990436077118, Accuracy: 86.33%\n",
      "Iteration: 400, Loss: 0.3990320861339569, Accuracy: 82.69%\n",
      "Iteration: 450, Loss: 0.6188592910766602, Accuracy: 85.53%\n",
      "Iteration: 500, Loss: 0.4051140248775482, Accuracy: 87.31%\n",
      "Iteration: 550, Loss: 0.3517197370529175, Accuracy: 88.22%\n",
      "Iteration: 600, Loss: 0.2855070233345032, Accuracy: 86.78%\n",
      "Iteration: 650, Loss: 0.29856136441230774, Accuracy: 88.45%\n",
      "Iteration: 700, Loss: 0.2577352821826935, Accuracy: 88.8%\n",
      "Iteration: 750, Loss: 0.370523601770401, Accuracy: 86.62%\n",
      "Iteration: 800, Loss: 0.2858850955963135, Accuracy: 87.93%\n",
      "Iteration: 850, Loss: 0.32631218433380127, Accuracy: 86.91%\n",
      "Iteration: 900, Loss: 0.3313886523246765, Accuracy: 87.62%\n",
      "Iteration: 950, Loss: 0.34314361214637756, Accuracy: 87.58%\n",
      "Iteration: 1000, Loss: 0.29351434111595154, Accuracy: 88.96%\n",
      "Iteration: 1050, Loss: 0.49584880471229553, Accuracy: 86.39%\n",
      "Iteration: 1100, Loss: 0.33115342259407043, Accuracy: 88.2%\n",
      "Iteration: 1150, Loss: 0.37123599648475647, Accuracy: 88.67%\n",
      "Iteration: 1200, Loss: 0.22729186713695526, Accuracy: 87.5%\n",
      "Iteration: 1250, Loss: 0.235828697681427, Accuracy: 89.35%\n",
      "Iteration: 1300, Loss: 0.19794057309627533, Accuracy: 89.68%\n",
      "Iteration: 1350, Loss: 0.3143293261528015, Accuracy: 88.41%\n",
      "Iteration: 1400, Loss: 0.21514897048473358, Accuracy: 88.48%\n",
      "Iteration: 1450, Loss: 0.25585776567459106, Accuracy: 88.7%\n",
      "Iteration: 1500, Loss: 0.20756739377975464, Accuracy: 89.11%\n",
      "Iteration: 1550, Loss: 0.2230469286441803, Accuracy: 89.25%\n",
      "Iteration: 1600, Loss: 0.32204481959342957, Accuracy: 90.3%\n",
      "Iteration: 1650, Loss: 0.45575112104415894, Accuracy: 87.41%\n",
      "Iteration: 1700, Loss: 0.26080530881881714, Accuracy: 90.26%\n",
      "Iteration: 1750, Loss: 0.3084790110588074, Accuracy: 89.55%\n",
      "Iteration: 1800, Loss: 0.26172053813934326, Accuracy: 87.83%\n",
      "Iteration: 1850, Loss: 0.21053357422351837, Accuracy: 89.41%\n",
      "Iteration: 1900, Loss: 0.1691821813583374, Accuracy: 89.56%\n",
      "Iteration: 1950, Loss: 0.30787357687950134, Accuracy: 89.73%\n",
      "Iteration: 2000, Loss: 0.20744848251342773, Accuracy: 89.23%\n",
      "Iteration: 2050, Loss: 0.18518608808517456, Accuracy: 88.33%\n",
      "Iteration: 2100, Loss: 0.22662422060966492, Accuracy: 88.22%\n",
      "Iteration: 2150, Loss: 0.21911388635635376, Accuracy: 89.69%\n",
      "Iteration: 2200, Loss: 0.3043436110019684, Accuracy: 90.39%\n",
      "Iteration: 2250, Loss: 0.49708425998687744, Accuracy: 89.17%\n",
      "Iteration: 2300, Loss: 0.2697272002696991, Accuracy: 90.27%\n",
      "Iteration: 2350, Loss: 0.29601433873176575, Accuracy: 89.1%\n",
      "Iteration: 2400, Loss: 0.215383380651474, Accuracy: 88.66%\n",
      "Iteration: 2450, Loss: 0.19048111140727997, Accuracy: 90.27%\n",
      "Iteration: 2500, Loss: 0.13517732918262482, Accuracy: 89.77%\n",
      "Iteration: 2550, Loss: 0.28451278805732727, Accuracy: 90.39%\n",
      "Iteration: 2600, Loss: 0.18254154920578003, Accuracy: 89.55%\n",
      "Iteration: 2650, Loss: 0.1899886578321457, Accuracy: 87.81%\n",
      "Iteration: 2700, Loss: 0.16270984709262848, Accuracy: 89.56%\n",
      "Iteration: 2750, Loss: 0.1889590471982956, Accuracy: 89.41%\n",
      "Iteration: 2800, Loss: 0.2724502682685852, Accuracy: 90.33%\n",
      "Iteration: 2850, Loss: 0.5269961953163147, Accuracy: 90.25%\n",
      "Iteration: 2900, Loss: 0.2573798894882202, Accuracy: 90.37%\n",
      "Iteration: 2950, Loss: 0.2791450023651123, Accuracy: 89.25%\n",
      "Iteration: 3000, Loss: 0.18946996331214905, Accuracy: 90.7%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        train = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 train_loader와 맞추기 위해 수정\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    test = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 test_loader와 맞추기 위해 수정\n",
    "                    outputs = model(test)\n",
    "                    predictions = torch.max(outputs, 1)[1]\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.item())\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SKT01\\anaconda3\\envs\\torch_book\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50/3000, Loss: 0.16564151644706726, Accuracy: 91.09%\n",
      "Iteration: 100/3000, Loss: 0.12494651973247528, Accuracy: 90.09%\n",
      "Iteration: 150/3000, Loss: 0.23480354249477386, Accuracy: 90.58%\n",
      "Iteration: 200/3000, Loss: 0.22143825888633728, Accuracy: 89.77%\n",
      "Iteration: 250/3000, Loss: 0.20013116300106049, Accuracy: 89.21%\n",
      "Iteration: 300/3000, Loss: 0.1350240856409073, Accuracy: 90.18%\n",
      "Iteration: 350/3000, Loss: 0.19082732498645782, Accuracy: 90.01%\n",
      "Iteration: 400/3000, Loss: 0.2076500654220581, Accuracy: 90.43%\n",
      "Iteration: 450/3000, Loss: 0.44753074645996094, Accuracy: 90.63%\n",
      "Iteration: 500/3000, Loss: 0.22584658861160278, Accuracy: 90.78%\n",
      "Iteration: 550/3000, Loss: 0.21136701107025146, Accuracy: 90.02%\n",
      "Iteration: 600/3000, Loss: 0.1873020976781845, Accuracy: 90.51%\n",
      "Iteration: 650/3000, Loss: 0.1611260026693344, Accuracy: 90.81%\n",
      "Iteration: 700/3000, Loss: 0.10839495062828064, Accuracy: 90.73%\n",
      "Iteration: 750/3000, Loss: 0.26778653264045715, Accuracy: 90.69%\n",
      "Iteration: 800/3000, Loss: 0.1871948391199112, Accuracy: 89.39%\n",
      "Iteration: 850/3000, Loss: 0.18903547525405884, Accuracy: 89.03%\n",
      "Iteration: 900/3000, Loss: 0.13062475621700287, Accuracy: 90.02%\n",
      "Iteration: 950/3000, Loss: 0.1124214455485344, Accuracy: 90.53%\n",
      "Iteration: 1000/3000, Loss: 0.1735944151878357, Accuracy: 90.38%\n",
      "Iteration: 1050/3000, Loss: 0.44174689054489136, Accuracy: 90.92%\n",
      "Iteration: 1100/3000, Loss: 0.22387483716011047, Accuracy: 90.79%\n",
      "Iteration: 1150/3000, Loss: 0.1624123901128769, Accuracy: 90.03%\n",
      "Iteration: 1200/3000, Loss: 0.2057773768901825, Accuracy: 90.32%\n",
      "Iteration: 1250/3000, Loss: 0.1480873078107834, Accuracy: 90.99%\n",
      "Iteration: 1300/3000, Loss: 0.13132329285144806, Accuracy: 90.02%\n",
      "Iteration: 1350/3000, Loss: 0.27439644932746887, Accuracy: 90.32%\n",
      "Iteration: 1400/3000, Loss: 0.19503575563430786, Accuracy: 90.69%\n",
      "Iteration: 1450/3000, Loss: 0.16940957307815552, Accuracy: 90.31%\n",
      "Iteration: 1500/3000, Loss: 0.13424742221832275, Accuracy: 90.0%\n",
      "Iteration: 1550/3000, Loss: 0.1508147418498993, Accuracy: 90.38%\n",
      "Iteration: 1600/3000, Loss: 0.17954395711421967, Accuracy: 90.08%\n",
      "Iteration: 1650/3000, Loss: 0.38792309165000916, Accuracy: 90.54%\n",
      "Iteration: 1700/3000, Loss: 0.17549076676368713, Accuracy: 91.15%\n",
      "Iteration: 1750/3000, Loss: 0.15355725586414337, Accuracy: 90.02%\n",
      "Iteration: 1800/3000, Loss: 0.1493741273880005, Accuracy: 89.49%\n",
      "Iteration: 1850/3000, Loss: 0.12372234463691711, Accuracy: 91.15%\n",
      "Iteration: 1900/3000, Loss: 0.14898939430713654, Accuracy: 90.37%\n",
      "Iteration: 1950/3000, Loss: 0.23904158174991608, Accuracy: 90.11%\n",
      "Iteration: 2000/3000, Loss: 0.24770675599575043, Accuracy: 90.51%\n",
      "Iteration: 2050/3000, Loss: 0.14147521555423737, Accuracy: 90.45%\n",
      "Iteration: 2100/3000, Loss: 0.12205103784799576, Accuracy: 89.92%\n",
      "Iteration: 2150/3000, Loss: 0.10450812429189682, Accuracy: 90.5%\n",
      "Iteration: 2200/3000, Loss: 0.132581889629364, Accuracy: 90.48%\n",
      "Iteration: 2250/3000, Loss: 0.33792904019355774, Accuracy: 90.93%\n",
      "Iteration: 2300/3000, Loss: 0.16355161368846893, Accuracy: 90.46%\n",
      "Iteration: 2350/3000, Loss: 0.11381959915161133, Accuracy: 90.36%\n",
      "Iteration: 2400/3000, Loss: 0.20144617557525635, Accuracy: 89.82%\n",
      "Iteration: 2450/3000, Loss: 0.15080980956554413, Accuracy: 90.66%\n",
      "Iteration: 2500/3000, Loss: 0.1027832105755806, Accuracy: 90.62%\n",
      "Iteration: 2550/3000, Loss: 0.21586070954799652, Accuracy: 89.94%\n",
      "Iteration: 2600/3000, Loss: 0.17506563663482666, Accuracy: 90.85%\n",
      "Iteration: 2650/3000, Loss: 0.12476838380098343, Accuracy: 90.65%\n",
      "Iteration: 2700/3000, Loss: 0.11502088606357574, Accuracy: 90.37%\n",
      "Iteration: 2750/3000, Loss: 0.08282741904258728, Accuracy: 90.22%\n",
      "Iteration: 2800/3000, Loss: 0.12070997059345245, Accuracy: 90.51%\n",
      "Iteration: 2850/3000, Loss: 0.3044474720954895, Accuracy: 90.81%\n",
      "Iteration: 2900/3000, Loss: 0.14405658841133118, Accuracy: 90.38%\n",
      "Iteration: 2950/3000, Loss: 0.08723082393407822, Accuracy: 89.71%\n",
      "Iteration: 3000/3000, Loss: 0.16451936960220337, Accuracy: 90.3%\n",
      "Best model saved with accuracy: 91.15%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_wts = None\n",
    "\n",
    "# 훈련 데이터 크기와 배치 크기\n",
    "train_data_size = len(train_loader.dataset)\n",
    "batch_size = train_loader.batch_size\n",
    "\n",
    "# iteration 수 계산\n",
    "total_iterations = num_epochs * (train_data_size // batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        train = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 train_loader와 맞추기 위해 수정\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    test = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 test_loader와 맞추기 위해 수정\n",
    "                    outputs = model(test)\n",
    "                    predictions = torch.max(outputs, 1)[1]\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.item())\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f\"Iteration: {count}/{total_iterations}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "            # 가장 높은 정확도를 기록한 모델 저장\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "# 최종 모델 저장\n",
    "if best_model_wts is not None:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy}%\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newFashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(newFashionDNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128*3*3, out_features=256)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newFashionDNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "model = newFashionDNN()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50/3000, Loss: 0.5421976447105408, Accuracy: 79.53%\n",
      "Iteration: 100/3000, Loss: 0.5422972440719604, Accuracy: 80.81%\n",
      "Iteration: 150/3000, Loss: 0.47840631008148193, Accuracy: 82.54%\n",
      "Iteration: 200/3000, Loss: 0.4011068642139435, Accuracy: 84.73%\n",
      "Iteration: 250/3000, Loss: 0.30903682112693787, Accuracy: 85.39%\n",
      "Iteration: 300/3000, Loss: 0.44430840015411377, Accuracy: 83.43%\n",
      "Iteration: 350/3000, Loss: 0.35734233260154724, Accuracy: 86.07%\n",
      "Iteration: 400/3000, Loss: 0.398668110370636, Accuracy: 87.19%\n",
      "Iteration: 450/3000, Loss: 0.49934250116348267, Accuracy: 86.15%\n",
      "Iteration: 500/3000, Loss: 0.4159265160560608, Accuracy: 87.25%\n",
      "Iteration: 550/3000, Loss: 0.2948434352874756, Accuracy: 86.96%\n",
      "Iteration: 600/3000, Loss: 0.28693127632141113, Accuracy: 86.27%\n",
      "Iteration: 650/3000, Loss: 0.29102623462677, Accuracy: 88.31%\n",
      "Iteration: 700/3000, Loss: 0.2072339802980423, Accuracy: 88.74%\n",
      "Iteration: 750/3000, Loss: 0.3002639412879944, Accuracy: 87.82%\n",
      "Iteration: 800/3000, Loss: 0.21355487406253815, Accuracy: 89.28%\n",
      "Iteration: 850/3000, Loss: 0.3102620542049408, Accuracy: 88.04%\n",
      "Iteration: 900/3000, Loss: 0.3753207325935364, Accuracy: 87.26%\n",
      "Iteration: 950/3000, Loss: 0.2253694385290146, Accuracy: 88.35%\n",
      "Iteration: 1000/3000, Loss: 0.2826938331127167, Accuracy: 89.56%\n",
      "Iteration: 1050/3000, Loss: 0.5007340908050537, Accuracy: 89.64%\n",
      "Iteration: 1100/3000, Loss: 0.3789573013782501, Accuracy: 89.77%\n",
      "Iteration: 1150/3000, Loss: 0.23010864853858948, Accuracy: 88.97%\n",
      "Iteration: 1200/3000, Loss: 0.16996970772743225, Accuracy: 89.47%\n",
      "Iteration: 1250/3000, Loss: 0.24643364548683167, Accuracy: 90.38%\n",
      "Iteration: 1300/3000, Loss: 0.20150917768478394, Accuracy: 89.81%\n",
      "Iteration: 1350/3000, Loss: 0.2553674280643463, Accuracy: 89.98%\n",
      "Iteration: 1400/3000, Loss: 0.1700296401977539, Accuracy: 89.74%\n",
      "Iteration: 1450/3000, Loss: 0.29817941784858704, Accuracy: 89.94%\n",
      "Iteration: 1500/3000, Loss: 0.2298370897769928, Accuracy: 88.99%\n",
      "Iteration: 1550/3000, Loss: 0.2223905622959137, Accuracy: 89.92%\n",
      "Iteration: 1600/3000, Loss: 0.2922293543815613, Accuracy: 90.28%\n",
      "Iteration: 1650/3000, Loss: 0.3813885748386383, Accuracy: 90.33%\n",
      "Iteration: 1700/3000, Loss: 0.2887852191925049, Accuracy: 90.44%\n",
      "Iteration: 1750/3000, Loss: 0.21021945774555206, Accuracy: 89.66%\n",
      "Iteration: 1800/3000, Loss: 0.1698407530784607, Accuracy: 89.81%\n",
      "Iteration: 1850/3000, Loss: 0.2279457151889801, Accuracy: 90.76%\n",
      "Iteration: 1900/3000, Loss: 0.10829197615385056, Accuracy: 90.44%\n",
      "Iteration: 1950/3000, Loss: 0.23956120014190674, Accuracy: 90.22%\n",
      "Iteration: 2000/3000, Loss: 0.2164023369550705, Accuracy: 90.07%\n",
      "Iteration: 2050/3000, Loss: 0.21555471420288086, Accuracy: 90.23%\n",
      "Iteration: 2100/3000, Loss: 0.1459704041481018, Accuracy: 89.64%\n",
      "Iteration: 2150/3000, Loss: 0.18154014647006989, Accuracy: 90.24%\n",
      "Iteration: 2200/3000, Loss: 0.2851751744747162, Accuracy: 90.24%\n",
      "Iteration: 2250/3000, Loss: 0.3636340796947479, Accuracy: 90.8%\n",
      "Iteration: 2300/3000, Loss: 0.2292846292257309, Accuracy: 90.92%\n",
      "Iteration: 2350/3000, Loss: 0.1458297222852707, Accuracy: 89.98%\n",
      "Iteration: 2400/3000, Loss: 0.19385568797588348, Accuracy: 90.12%\n",
      "Iteration: 2450/3000, Loss: 0.189630389213562, Accuracy: 90.89%\n",
      "Iteration: 2500/3000, Loss: 0.14380882680416107, Accuracy: 90.53%\n",
      "Iteration: 2550/3000, Loss: 0.1763131469488144, Accuracy: 90.34%\n",
      "Iteration: 2600/3000, Loss: 0.15068869292736053, Accuracy: 90.4%\n",
      "Iteration: 2650/3000, Loss: 0.20430080592632294, Accuracy: 90.56%\n",
      "Iteration: 2700/3000, Loss: 0.0937037244439125, Accuracy: 90.45%\n",
      "Iteration: 2750/3000, Loss: 0.18255329132080078, Accuracy: 90.3%\n",
      "Iteration: 2800/3000, Loss: 0.21967679262161255, Accuracy: 90.67%\n",
      "Iteration: 2850/3000, Loss: 0.38444122672080994, Accuracy: 91.2%\n",
      "Iteration: 2900/3000, Loss: 0.18398813903331757, Accuracy: 91.49%\n",
      "Iteration: 2950/3000, Loss: 0.14896449446678162, Accuracy: 90.7%\n",
      "Iteration: 3000/3000, Loss: 0.17224770784378052, Accuracy: 90.5%\n",
      "Best model saved with accuracy: 91.49%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_wts = None\n",
    "\n",
    "# 훈련 데이터 크기와 배치 크기\n",
    "train_data_size = len(train_loader.dataset)\n",
    "batch_size = train_loader.batch_size\n",
    "\n",
    "# iteration 수 계산\n",
    "total_iterations = num_epochs * (train_data_size // batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        train = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 train_loader와 맞추기 위해 수정\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    test = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 test_loader와 맞추기 위해 수정\n",
    "                    outputs = model(test)\n",
    "                    predictions = torch.max(outputs, 1)[1]\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.item())\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f\"Iteration: {count}/{total_iterations}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "            # 가장 높은 정확도를 기록한 모델 저장\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "# 최종 모델 저장\n",
    "if best_model_wts is not None:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy}%\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestFashionCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BestFashionCNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestFashionCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "model = BestFashionCNN()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50/3000, Loss: 0.5406960248947144, Accuracy: 79.64%\n",
      "Iteration: 100/3000, Loss: 0.46793416142463684, Accuracy: 80.31%\n",
      "Iteration: 150/3000, Loss: 0.4378383159637451, Accuracy: 84.9%\n",
      "Iteration: 200/3000, Loss: 0.3999532461166382, Accuracy: 85.45%\n",
      "Iteration: 250/3000, Loss: 0.4046562910079956, Accuracy: 86.19%\n",
      "Iteration: 300/3000, Loss: 0.4444766640663147, Accuracy: 85.83%\n",
      "Iteration: 350/3000, Loss: 0.34449490904808044, Accuracy: 84.55%\n",
      "Iteration: 400/3000, Loss: 0.4054742753505707, Accuracy: 85.48%\n",
      "Iteration: 450/3000, Loss: 0.6262008547782898, Accuracy: 84.82%\n",
      "Iteration: 500/3000, Loss: 0.4640432596206665, Accuracy: 87.94%\n",
      "Iteration: 550/3000, Loss: 0.4052322506904602, Accuracy: 88.08%\n",
      "Iteration: 600/3000, Loss: 0.24692806601524353, Accuracy: 86.58%\n",
      "Iteration: 650/3000, Loss: 0.2659952938556671, Accuracy: 88.9%\n",
      "Iteration: 700/3000, Loss: 0.2456912398338318, Accuracy: 88.52%\n",
      "Iteration: 750/3000, Loss: 0.3559759855270386, Accuracy: 87.03%\n",
      "Iteration: 800/3000, Loss: 0.3494836688041687, Accuracy: 88.32%\n",
      "Iteration: 850/3000, Loss: 0.2681475579738617, Accuracy: 88.36%\n",
      "Iteration: 900/3000, Loss: 0.29678255319595337, Accuracy: 88.8%\n",
      "Iteration: 950/3000, Loss: 0.30812937021255493, Accuracy: 87.91%\n",
      "Iteration: 1000/3000, Loss: 0.25843724608421326, Accuracy: 88.35%\n",
      "Iteration: 1050/3000, Loss: 0.4316876530647278, Accuracy: 86.44%\n",
      "Iteration: 1100/3000, Loss: 0.31096911430358887, Accuracy: 89.29%\n",
      "Iteration: 1150/3000, Loss: 0.410194993019104, Accuracy: 88.26%\n",
      "Iteration: 1200/3000, Loss: 0.20017828047275543, Accuracy: 87.92%\n",
      "Iteration: 1250/3000, Loss: 0.2176932692527771, Accuracy: 89.57%\n",
      "Iteration: 1300/3000, Loss: 0.17453250288963318, Accuracy: 88.8%\n",
      "Iteration: 1350/3000, Loss: 0.3012195825576782, Accuracy: 88.6%\n",
      "Iteration: 1400/3000, Loss: 0.21756675839424133, Accuracy: 88.87%\n",
      "Iteration: 1450/3000, Loss: 0.23904949426651, Accuracy: 89.18%\n",
      "Iteration: 1500/3000, Loss: 0.2847403287887573, Accuracy: 88.5%\n",
      "Iteration: 1550/3000, Loss: 0.2701753079891205, Accuracy: 87.74%\n",
      "Iteration: 1600/3000, Loss: 0.3156391978263855, Accuracy: 89.75%\n",
      "Iteration: 1650/3000, Loss: 0.4208572804927826, Accuracy: 87.18%\n",
      "Iteration: 1700/3000, Loss: 0.2913303077220917, Accuracy: 90.12%\n",
      "Iteration: 1750/3000, Loss: 0.3479739725589752, Accuracy: 88.95%\n",
      "Iteration: 1800/3000, Loss: 0.19557824730873108, Accuracy: 89.06%\n",
      "Iteration: 1850/3000, Loss: 0.16519278287887573, Accuracy: 90.58%\n",
      "Iteration: 1900/3000, Loss: 0.15105798840522766, Accuracy: 89.65%\n",
      "Iteration: 1950/3000, Loss: 0.2840574383735657, Accuracy: 89.75%\n",
      "Iteration: 2000/3000, Loss: 0.18752610683441162, Accuracy: 89.24%\n",
      "Iteration: 2050/3000, Loss: 0.2075205296278, Accuracy: 89.18%\n",
      "Iteration: 2100/3000, Loss: 0.25793930888175964, Accuracy: 88.02%\n",
      "Iteration: 2150/3000, Loss: 0.2137688845396042, Accuracy: 90.06%\n",
      "Iteration: 2200/3000, Loss: 0.2687144875526428, Accuracy: 89.57%\n",
      "Iteration: 2250/3000, Loss: 0.4246000647544861, Accuracy: 89.07%\n",
      "Iteration: 2300/3000, Loss: 0.2775406539440155, Accuracy: 90.43%\n",
      "Iteration: 2350/3000, Loss: 0.3024834394454956, Accuracy: 88.97%\n",
      "Iteration: 2400/3000, Loss: 0.1994183361530304, Accuracy: 89.4%\n",
      "Iteration: 2450/3000, Loss: 0.14571146667003632, Accuracy: 91.14%\n",
      "Iteration: 2500/3000, Loss: 0.1356257200241089, Accuracy: 89.67%\n",
      "Iteration: 2550/3000, Loss: 0.24515600502490997, Accuracy: 89.89%\n",
      "Iteration: 2600/3000, Loss: 0.15816865861415863, Accuracy: 89.01%\n",
      "Iteration: 2650/3000, Loss: 0.21257582306861877, Accuracy: 88.71%\n",
      "Iteration: 2700/3000, Loss: 0.17927759885787964, Accuracy: 89.7%\n",
      "Iteration: 2750/3000, Loss: 0.19054903090000153, Accuracy: 90.23%\n",
      "Iteration: 2800/3000, Loss: 0.22400473058223724, Accuracy: 89.65%\n",
      "Iteration: 2850/3000, Loss: 0.4246334433555603, Accuracy: 89.85%\n",
      "Iteration: 2900/3000, Loss: 0.24441051483154297, Accuracy: 90.46%\n",
      "Iteration: 2950/3000, Loss: 0.34839627146720886, Accuracy: 89.07%\n",
      "Iteration: 3000/3000, Loss: 0.21455371379852295, Accuracy: 90.38%\n",
      "Best model saved with accuracy: 91.14%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_wts = None\n",
    "\n",
    "# 훈련 데이터 크기와 배치 크기\n",
    "train_data_size = len(train_loader.dataset)\n",
    "batch_size = train_loader.batch_size\n",
    "\n",
    "# iteration 수 계산\n",
    "total_iterations = num_epochs * (train_data_size // batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        train = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 train_loader와 맞추기 위해 수정\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    test = images.view(images.size(0), 1, 28, 28)  # 배치 크기를 test_loader와 맞추기 위해 수정\n",
    "                    outputs = model(test)\n",
    "                    predictions = torch.max(outputs, 1)[1]\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.item())\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f\"Iteration: {count}/{total_iterations}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "            # 가장 높은 정확도를 기록한 모델 저장\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "# 최종 모델 저장\n",
    "if best_model_wts is not None:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy}%\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50/4685, Loss: 0.45976966619491577, Accuracy: 73.46%\n",
      "Iteration: 100/4685, Loss: 0.6057692766189575, Accuracy: 80.88%\n",
      "Iteration: 150/4685, Loss: 0.43997037410736084, Accuracy: 82.66%\n",
      "Iteration: 200/4685, Loss: 0.4657905697822571, Accuracy: 83.9%\n",
      "Iteration: 250/4685, Loss: 0.38294345140457153, Accuracy: 85.9%\n",
      "Iteration: 300/4685, Loss: 0.19038014113903046, Accuracy: 85.55%\n",
      "Iteration: 350/4685, Loss: 0.3014202117919922, Accuracy: 86.27%\n",
      "Iteration: 400/4685, Loss: 0.49786195158958435, Accuracy: 85.94%\n",
      "Iteration: 450/4685, Loss: 0.2876548767089844, Accuracy: 87.28%\n",
      "Iteration: 500/4685, Loss: 0.38239458203315735, Accuracy: 87.52%\n",
      "Iteration: 550/4685, Loss: 0.4319179952144623, Accuracy: 88.09%\n",
      "Iteration: 600/4685, Loss: 0.29250869154930115, Accuracy: 88.0%\n",
      "Iteration: 650/4685, Loss: 0.16631552577018738, Accuracy: 88.19%\n",
      "Iteration: 700/4685, Loss: 0.3579227030277252, Accuracy: 87.63%\n",
      "Iteration: 750/4685, Loss: 0.3134734332561493, Accuracy: 87.12%\n",
      "Iteration: 800/4685, Loss: 0.1630764901638031, Accuracy: 88.69%\n",
      "Iteration: 850/4685, Loss: 0.4359215497970581, Accuracy: 86.9%\n",
      "Iteration: 900/4685, Loss: 0.3906134068965912, Accuracy: 89.09%\n",
      "Iteration: 950/4685, Loss: 0.5961795449256897, Accuracy: 87.09%\n",
      "Iteration: 1000/4685, Loss: 0.3639519512653351, Accuracy: 88.14%\n",
      "Iteration: 1050/4685, Loss: 0.21373426914215088, Accuracy: 88.43%\n",
      "Iteration: 1100/4685, Loss: 0.42484837770462036, Accuracy: 89.21%\n",
      "Iteration: 1150/4685, Loss: 0.12057547271251678, Accuracy: 88.31%\n",
      "Iteration: 1200/4685, Loss: 0.2018590271472931, Accuracy: 88.52%\n",
      "Iteration: 1250/4685, Loss: 0.31009525060653687, Accuracy: 89.27%\n",
      "Iteration: 1300/4685, Loss: 0.39388829469680786, Accuracy: 89.41%\n",
      "Iteration: 1350/4685, Loss: 0.17523202300071716, Accuracy: 89.13%\n",
      "Iteration: 1400/4685, Loss: 0.22495704889297485, Accuracy: 88.74%\n",
      "Iteration: 1450/4685, Loss: 0.273097962141037, Accuracy: 89.91%\n",
      "Iteration: 1500/4685, Loss: 0.22400903701782227, Accuracy: 88.25%\n",
      "Iteration: 1550/4685, Loss: 0.20857243239879608, Accuracy: 89.06%\n",
      "Iteration: 1600/4685, Loss: 0.17428632080554962, Accuracy: 88.8%\n",
      "Iteration: 1650/4685, Loss: 0.23185217380523682, Accuracy: 89.77%\n",
      "Iteration: 1700/4685, Loss: 0.260408878326416, Accuracy: 90.44%\n",
      "Iteration: 1750/4685, Loss: 0.20900137722492218, Accuracy: 89.36%\n",
      "Iteration: 1800/4685, Loss: 0.4459572434425354, Accuracy: 89.66%\n",
      "Iteration: 1850/4685, Loss: 0.14362432062625885, Accuracy: 90.21%\n",
      "Iteration: 1900/4685, Loss: 0.3455636501312256, Accuracy: 86.95%\n",
      "Iteration: 1950/4685, Loss: 0.4434746503829956, Accuracy: 89.79%\n",
      "Iteration: 2000/4685, Loss: 0.25969547033309937, Accuracy: 87.32%\n",
      "Iteration: 2050/4685, Loss: 0.37329617142677307, Accuracy: 89.58%\n",
      "Iteration: 2100/4685, Loss: 0.17124035954475403, Accuracy: 89.66%\n",
      "Iteration: 2150/4685, Loss: 0.26207640767097473, Accuracy: 89.78%\n",
      "Iteration: 2200/4685, Loss: 0.1600429266691208, Accuracy: 90.32%\n",
      "Iteration: 2250/4685, Loss: 0.20994403958320618, Accuracy: 90.37%\n",
      "Iteration: 2300/4685, Loss: 0.2882210314273834, Accuracy: 89.47%\n",
      "Iteration: 2350/4685, Loss: 0.33616605401039124, Accuracy: 90.27%\n",
      "Iteration: 2400/4685, Loss: 0.25114428997039795, Accuracy: 90.27%\n",
      "Iteration: 2450/4685, Loss: 0.307046115398407, Accuracy: 89.6%\n",
      "Iteration: 2500/4685, Loss: 0.19400560855865479, Accuracy: 90.05%\n",
      "Iteration: 2550/4685, Loss: 0.18057793378829956, Accuracy: 89.71%\n",
      "Iteration: 2600/4685, Loss: 0.42640313506126404, Accuracy: 90.22%\n",
      "Iteration: 2650/4685, Loss: 0.10884243249893188, Accuracy: 89.82%\n",
      "Iteration: 2700/4685, Loss: 0.6576967239379883, Accuracy: 90.61%\n",
      "Iteration: 2750/4685, Loss: 0.2462056577205658, Accuracy: 89.39%\n",
      "Iteration: 2800/4685, Loss: 0.2327953279018402, Accuracy: 90.32%\n",
      "Iteration: 2850/4685, Loss: 0.29848790168762207, Accuracy: 88.93%\n",
      "Iteration: 2900/4685, Loss: 0.23741614818572998, Accuracy: 89.91%\n",
      "Iteration: 2950/4685, Loss: 0.2580861747264862, Accuracy: 90.4%\n",
      "Iteration: 3000/4685, Loss: 0.14574792981147766, Accuracy: 89.8%\n",
      "Iteration: 3050/4685, Loss: 0.1283147782087326, Accuracy: 89.99%\n",
      "Iteration: 3100/4685, Loss: 0.14302513003349304, Accuracy: 90.06%\n",
      "Iteration: 3150/4685, Loss: 0.19352661073207855, Accuracy: 90.35%\n",
      "Iteration: 3200/4685, Loss: 0.19442641735076904, Accuracy: 90.75%\n",
      "Iteration: 3250/4685, Loss: 0.30019912123680115, Accuracy: 90.55%\n",
      "Iteration: 3300/4685, Loss: 0.10245218873023987, Accuracy: 90.51%\n",
      "Iteration: 3350/4685, Loss: 0.21871104836463928, Accuracy: 90.54%\n",
      "Iteration: 3400/4685, Loss: 0.1669207066297531, Accuracy: 89.84%\n",
      "Iteration: 3450/4685, Loss: 0.16561681032180786, Accuracy: 90.19%\n",
      "Iteration: 3500/4685, Loss: 0.2071635127067566, Accuracy: 90.2%\n",
      "Iteration: 3550/4685, Loss: 0.22722381353378296, Accuracy: 90.73%\n",
      "Iteration: 3600/4685, Loss: 0.15413209795951843, Accuracy: 90.12%\n",
      "Iteration: 3650/4685, Loss: 0.25012102723121643, Accuracy: 91.02%\n",
      "Iteration: 3700/4685, Loss: 0.08019749820232391, Accuracy: 90.28%\n",
      "Iteration: 3750/4685, Loss: 0.33970391750335693, Accuracy: 90.12%\n",
      "Iteration: 3800/4685, Loss: 0.4254802465438843, Accuracy: 89.29%\n",
      "Iteration: 3850/4685, Loss: 0.17466343939304352, Accuracy: 90.56%\n",
      "Iteration: 3900/4685, Loss: 0.2657870054244995, Accuracy: 90.77%\n",
      "Iteration: 3950/4685, Loss: 0.14425282180309296, Accuracy: 89.39%\n",
      "Iteration: 4000/4685, Loss: 0.27820920944213867, Accuracy: 90.47%\n",
      "Iteration: 4050/4685, Loss: 0.22864820063114166, Accuracy: 90.04%\n",
      "Iteration: 4100/4685, Loss: 0.1313837170600891, Accuracy: 89.83%\n",
      "Iteration: 4150/4685, Loss: 0.1844564974308014, Accuracy: 90.4%\n",
      "Iteration: 4200/4685, Loss: 0.06527366489171982, Accuracy: 90.16%\n",
      "Iteration: 4250/4685, Loss: 0.2598861753940582, Accuracy: 90.53%\n",
      "Iteration: 4300/4685, Loss: 0.17140191793441772, Accuracy: 90.76%\n",
      "Iteration: 4350/4685, Loss: 0.19687357544898987, Accuracy: 90.37%\n",
      "Iteration: 4400/4685, Loss: 0.19519712030887604, Accuracy: 89.8%\n",
      "Iteration: 4450/4685, Loss: 0.10585204511880875, Accuracy: 90.91%\n",
      "Iteration: 4500/4685, Loss: 0.07760027050971985, Accuracy: 89.89%\n",
      "Iteration: 4550/4685, Loss: 0.39358454942703247, Accuracy: 90.66%\n",
      "Iteration: 4600/4685, Loss: 0.13065782189369202, Accuracy: 91.24%\n",
      "Iteration: 4650/4685, Loss: 0.2602226138114929, Accuracy: 91.3%\n",
      "Best model saved with accuracy: 91.3%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 최적화된 newFashionDNN 모델 정의\n",
    "class newFashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(newFashionDNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128*3*3, out_features=256)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# 데이터셋 및 데이터 로더 설정\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = newFashionDNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 스케줄러 초기화\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# 훈련 설정\n",
    "num_epochs = 5\n",
    "count = 0\n",
    "\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_wts = None\n",
    "\n",
    "# 훈련 데이터 크기와 배치 크기\n",
    "train_data_size = len(train_loader.dataset)\n",
    "\n",
    "# iteration 수 계산\n",
    "total_iterations = num_epochs * (train_data_size // batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            model.eval()  # 모델을 평가 모드로 설정\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    predictions = torch.max(outputs, 1)[1]\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.item())\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f\"Iteration: {count}/{total_iterations}, Loss: {loss.item()}, Accuracy: {accuracy}%\")\n",
    "\n",
    "            # 가장 높은 정확도를 기록한 모델 저장\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "    # 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "    model.train()  # 다음 epoch를 위해 모델을 다시 훈련 모드로 설정\n",
    "\n",
    "# 최종 모델 저장\n",
    "if best_model_wts is not None:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy}%\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
